{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parte I - Análisis de la base de hogares y cálculo de pobreza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Analisis exploratorio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comezamos pensando, solo intuitvamente cuales a priori son las variables que podrian ser utilies para predecir la pobreza y perfeccionar las estimaciones el TP2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dentro de la varibles en la encuesta de hogares consideramos relevantes las caracterisicas de la vivienda:\n",
    "* El tipo de vivienda (IV1)\n",
    "* Cantidad de habitaciones (IV2)\n",
    "* Material del techo (V4)\n",
    "* Revestimiento del techo (IV5)\n",
    "* Tipo de conexion a agua (IV6 y IV7)\n",
    "* Tener baño (IV8), tipo de baño (IV9) y equipamiento (IV10)\n",
    "* Tipo de desague (IV11)\n",
    "* Zona de ubicacion: cercano a basural (IV12_1), zona inundable (IV12_2) y/o villa de emergecia (IV12_3)\n",
    "\n",
    "\n",
    "Las caractersiticas habitacionales:\n",
    "* Cantidad de ambientes (II1)\n",
    "* Cantidad de dormitorios (II2)\n",
    "* Habitaccion para trabajo (II3), exlusivamente (II6)\n",
    "* Tienen: cuarto de cocina (II4_1), lavadero (II4_2), garage (II4_3), cuantos de estos se usan para dormir (II5).\n",
    "* Regimen de tenencia (II7)\n",
    "* Combustible para cocinar (II8)\n",
    "* Propiedad del baño (II9)\n",
    "\n",
    "Variables de tipo de ingresos: V1 a 18. Y variables de trabajo infantil: V19_A y V19_B\n",
    "\n",
    "Integrantes del hogar: total (IX_Tot) y menores de 10 (IX_men10).\n",
    "\n",
    "No icluimos las que son mediciones de ingresos al igual que en el TP2.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Continuamos descargando la base de hogares y cargandola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos las librerias necesarias\n",
    "import os  \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt  # Para matriz de correlaciones\n",
    "import statsmodels.api as sm     \n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression, Lasso, LassoCV, Ridge, RidgeCV\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV, RepeatedKFold\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix, accuracy_score, roc_curve, RocCurveDisplay, average_precision_score,roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.special import expit\n",
    "\n",
    "# Definimos el directorio\n",
    "os.chdir('C:/Users/Usuario/Desktop/MAESTRIA/Big Data/TPs/BigData/TP3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos los datos, menteniendo solo las observaciones para el Gran Buenis Aires y la Ciudad de Buenos Aires:\n",
    "hogares = pd.read_excel(\"inputs/usu_hogar_T123.xlsx\")\n",
    "hogares = hogares[(hogares['AGLOMERADO']==32) | (hogares['AGLOMERADO']==33)]\n",
    "individual = pd.read_excel(\"inputs/usu_individual_T123.xlsx\")\n",
    "individual = individual[(individual['AGLOMERADO']==32) | (individual['AGLOMERADO']==33)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Unimos la tablas de Individuos y hogares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chequeamos que columnas estan duplicadas para que no se nos duplique en el merge\n",
    "columnas_duplicadas = set(hogares.columns).intersection(set(individual.columns))\n",
    "# Removemos CODUSU y NRO_HOGAR de la lista, para mantenerlas como ids del merge\n",
    "columnas_duplicadas.remove(\"CODUSU\")\n",
    "columnas_duplicadas.remove(\"NRO_HOGAR\")\n",
    "# Eliminamos los duplicados de la base hogar\n",
    "hogares.drop(columnas_duplicadas, axis=1, inplace=True)\n",
    "\n",
    "# Hacemos el left join de los hogares con los individuos\n",
    "df = pd.merge(individual,hogares, on=[\"CODUSU\", \"NRO_HOGAR\"], how=\"left\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Herramientas de limpieza de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la limpieza utilizaremos las herramientas que nos ofrece el modulo `pandas`, para el manejo de DataFrames. A continuación, explicaré las funciones que se están utilizando:\n",
    "\n",
    "   - la función `describe()` proporciona información resumida sobre la distribución de valores en esa columna. Incluyendo la media, minimo, desviación estánda, cuartiles, etc.\n",
    "\n",
    "   - `isnull()` es un método de los DataFrames de pandas que devuelve una matriz booleana indicando las ubicaciones de los valores faltantes en el DataFrame.\n",
    "\n",
    "   - `dropna()` es un método de los DataFrames de pandas que elimina las filas o columnas con valores faltantes.\n",
    "\n",
    "   - La función `fillna()` se utiliza para rellenar los valores faltantes en la columna.\n",
    "\n",
    "   - La función `drop()` con `axis=1`, elimina una columna inidicada.\n",
    "\n",
    "   -  La función `duplicated()` es un método de pandas que devuelve una Serie de valores booleanos que indica si cada fila del DataFrame es una duplicada de una fila anterior. Con `sum()` contamos estos booleanos generado de modo de obtener la cantidad de duplicados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Limpieza de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos un cheaqueo rapido de los datos, generando una tabla de estadisticas descriptivas\n",
    "descripcion = df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chequeamos que no haya duplicados\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Columna</th>\n",
       "      <th>Porcentaje de NAs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CODUSU</th>\n",
       "      <td>CODUSU</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ANO4</th>\n",
       "      <td>ANO4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TRIMESTRE</th>\n",
       "      <td>TRIMESTRE</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NRO_HOGAR</th>\n",
       "      <td>NRO_HOGAR</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COMPONENTE</th>\n",
       "      <td>COMPONENTE</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VII1_2</th>\n",
       "      <td>VII1_2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VII2_1</th>\n",
       "      <td>VII2_1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VII2_2</th>\n",
       "      <td>VII2_2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VII2_3</th>\n",
       "      <td>VII2_3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VII2_4</th>\n",
       "      <td>VII2_4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>242 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Columna  Porcentaje de NAs\n",
       "CODUSU          CODUSU                0.0\n",
       "ANO4              ANO4                0.0\n",
       "TRIMESTRE    TRIMESTRE                0.0\n",
       "NRO_HOGAR    NRO_HOGAR                0.0\n",
       "COMPONENTE  COMPONENTE                0.0\n",
       "...                ...                ...\n",
       "VII1_2          VII1_2                0.0\n",
       "VII2_1          VII2_1                0.0\n",
       "VII2_2          VII2_2                0.0\n",
       "VII2_3          VII2_3                0.0\n",
       "VII2_4          VII2_4                0.0\n",
       "\n",
       "[242 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Chequeamoss el porcentaje de missing values\n",
    "percent_missing = df.isnull().sum() * 100 / len(df)\n",
    "missing_value_eph = pd.DataFrame({'Columna': df.columns,\n",
    "                                 'Porcentaje de NAs': percent_missing})\n",
    "missing_value_eph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Eliminimamos la varaible si esta tiene mas de 90% de percent_missing\n",
    "df = df.dropna(thresh=len(df)*0.9, axis=1)\n",
    "# Chequeamos nuevamente los missing\n",
    "percent_missing = df.isnull().sum() * 100 / len(df)\n",
    "missing_value_eph = pd.DataFrame({'Columna': df.columns,\n",
    "                                 'Porcentaje de NAs': percent_missing})\n",
    "# Remplazamos los missings en CH08 (cobertura medica) por 9 = Ns/Nc\n",
    "df['CH08'] = df['CH08'].fillna(9)\n",
    "# Consideramos que el que sea missing brinda informacion relevante, por lo que lo dejamos como una categoria mas\n",
    "\n",
    "# De este modo no tenemos mas NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    7.619000e+03\n",
       "mean     1.384079e+05\n",
       "std      3.349417e+05\n",
       "min      0.000000e+00\n",
       "25%      0.000000e+00\n",
       "50%      7.500000e+04\n",
       "75%      2.000000e+05\n",
       "max      1.099000e+07\n",
       "Name: ITF, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cheaqueamos que no existan ingresos negativos\n",
    "df['ITF'].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cheqqueamos que no haya edades negativas\n",
    "df['CH06'].describe()\n",
    "\n",
    "# eliminamos observaciones con datos NS/NC en la variable para cantidad de habitaciones \n",
    "df = df[(df['IV2'] < 99)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por ultimo transformamos todas las variable categoricas a dummies\n",
    "for col in ['IV1', 'IV3', 'IV6', 'IV7', 'IV8', 'IV9', 'IV12_1', 'IV12_3', 'II7', 'II8', 'V1', 'V2', 'V5', 'DECCFR', 'CH09', 'CH10', 'CH11', 'CH12']:\n",
    "    df[col] = df[col].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Estadisticas descriptivas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación presentamos las estadisticas descriptivas, de cinco variable de la encuesta de hogares que consideramos relevantes: Total  de integrantes del hogar(IX_Tot), integrantes menores de 10 años (IX_men10), cantidad de habitaciones (IV2), vivienda en zona inundable (IV12_2), y vivienda en villa de emergecia (IV12_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IX_TOT</th>\n",
       "      <th>IX_MEN10</th>\n",
       "      <th>IV2</th>\n",
       "      <th>IV12_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2723.000000</td>\n",
       "      <td>2723.000000</td>\n",
       "      <td>2723.000000</td>\n",
       "      <td>2723.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.783694</td>\n",
       "      <td>0.345575</td>\n",
       "      <td>2.881014</td>\n",
       "      <td>1.923614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.594857</td>\n",
       "      <td>0.712493</td>\n",
       "      <td>1.124088</td>\n",
       "      <td>0.265664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            IX_TOT     IX_MEN10          IV2       IV12_2\n",
       "count  2723.000000  2723.000000  2723.000000  2723.000000\n",
       "mean      2.783694     0.345575     2.881014     1.923614\n",
       "std       1.594857     0.712493     1.124088     0.265664\n",
       "min       1.000000     0.000000     1.000000     1.000000\n",
       "25%       2.000000     0.000000     2.000000     2.000000\n",
       "50%       2.000000     0.000000     3.000000     2.000000\n",
       "75%       4.000000     0.000000     3.000000     2.000000\n",
       "max      13.000000     8.000000     9.000000     2.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eliminamos las observaciones de hogares que se duplican por individuo\n",
    "variables_interes = df.drop_duplicates(subset='CODUSU', keep='first')\n",
    "# Varaibles de interes\n",
    "variables_interes = variables_interes[['IX_TOT', 'IX_MEN10', 'IV2', 'IV12_2', 'IV12_3']]\n",
    "\n",
    "variables_interes.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que para los 2742 hogare en la emcuesta, en promedio  hay 3 integrantes por hogar, y que aproximadamente solo uno de cada tres hogares tienen un nicño menor a 10 años. En promedio hay 3 habitaciones por hogar, y solo el 2% de los hogares se encuentran en zonas inundables o en villas de emergencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Calculo de Adulto Equivalente y Requerimentos del Hogar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargaos la tabla de equivalencia de adultos\n",
    "adulto_equiv_data = pd.read_excel(\"inputs/tabla_adulto_equiv.xlsx\")\n",
    "\n",
    "# melt adulto_equiv_data Muejeres y Hombres\n",
    "adulto_equiv_data = pd.melt(adulto_equiv_data, id_vars=['Edad'], value_name= 'adulto_equiv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Change values of variable column to 1 if Hom and 2 if Muj\n",
    "adulto_equiv_data['variable'] = np.where(adulto_equiv_data['variable']=='Mujeres', 2, 1)\n",
    "# Rename variable column to Sexo\n",
    "adulto_equiv_data = adulto_equiv_data.rename(columns={'variable': 'Sexo'})\n",
    "\n",
    "#Función que lee los valores de edad en números CH06 y me lo impacta en la categoría etarea correspondiende de la nueva tabla.\n",
    "def rango_edad(edad):\n",
    "    if edad < 0:\n",
    "        rangoetareo = \"Menor de 1 años\"\n",
    "    elif edad > 0 and edad < 18:\n",
    "        rangoetareo = str(edad)+\" años\"\n",
    "    elif 17 < edad and edad < 30:\n",
    "        rangoetareo = \"18 a 29 años\"\n",
    "    elif 29 < edad and edad < 46:\n",
    "        rangoetareo = \"30 a 45 años\"\n",
    "    elif 45 < edad and edad < 61:\n",
    "        rangoetareo = \"46 a 60 años\"\n",
    "    elif 60 < edad and edad < 76:\n",
    "        rangoetareo = \"61 a 75 años\"\n",
    "    elif edad > 75:\n",
    "        rangoetareo = \"más de 75 años\"\n",
    "        \n",
    "    else:\n",
    "        rangoetareo = 'NaN'\n",
    "    return rangoetareo\n",
    "\n",
    "#Aplico la función a mi tabla para crear la columna deseada\n",
    "df['rango_etareo'] = df['CH06'].apply(rango_edad)\n",
    "\n",
    "\n",
    "#Renombro la columna \"edad\", igual que la de la otra tabla\n",
    "df = df.rename(columns={'rango_etareo': 'Edad'})\n",
    "df = df.rename(columns={'CH04': 'Sexo'})\n",
    "\n",
    "# Mejoramos la funcion de match en relacion al TP2 donde los match para los menores de 1 año no estaban bien definidos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hacemos el merge de las dos tablas\n",
    "df = df.merge(adulto_equiv_data, on=['Sexo','Edad'], how='left')\n",
    "#Sumo para las personas de un mismo hogary loguardo como ad_equiv_hogar\n",
    "df['ad_equiv_hogar'] = df.groupby('CODUSU')['adulto_equiv'].transform('sum')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Particiones segun respuesta y ingreso necesario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punto 1.3: Particionamos la muestra segun respondieron o no sobre los ingresos:\n",
    "respondieron = df[df['ITF'] > 0]\n",
    "respondieron = respondieron.reset_index(drop=True)\n",
    "norespondieron = df[df['ITF'] <= 0]\n",
    "norespondieron = norespondieron.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Punto 1.4: Agregar columna que indica el ingreso necesario del hogar para no ser pobre\n",
    "respondieron['ingreso_necesario'] = respondieron['ad_equiv_hogar'] * 53371.05\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Generamos una indicadora de persona pobre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34947768281101615\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Punto 1.4: Columna que indica si una persona es pobre según el ingreso de su hogar\n",
    "respondieron.loc[:, 'pobre'] = (respondieron['ingreso_necesario'] > respondieron['ITF']).astype(int)\n",
    "mean_pobres = respondieron['pobre'].mean()\n",
    "print(mean_pobres)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Calculo de pobreza por hogares (muestra expandida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23492852703542574\n"
     ]
    }
   ],
   "source": [
    "# Calaculamos la tasa de pobreza de hogares, expondiendo la muestra con el ponderador de hogares, PONDIH.\n",
    "# Columna indicadora de pobreza de hogares\n",
    "respondieron.loc[:, 'pobre_hogar'] = (respondieron['ingreso_necesario'] > respondieron['ITF']).astype(int)\n",
    "\n",
    "# Filtar por unico CODUSU\n",
    "respondieron = respondieron.drop_duplicates(subset=['CODUSU'])\n",
    "\n",
    "# Tasa de pobreza de hogares (sin expandir)\n",
    "mean_pobres_hogar_temp = respondieron['pobre_hogar'].mean()\n",
    "print(mean_pobres_hogar_temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26324299596014633\n"
     ]
    }
   ],
   "source": [
    "# Ponderar los hogares pobres\n",
    "respondieron['pobre_hogar_ponde'] = respondieron['PONDIH'] * respondieron['pobre_hogar']\n",
    "\n",
    "\n",
    "\n",
    "# Las guaradamos para reutilizarlas en la parte 3\n",
    "respondieron.to_csv('Inputs/df_respondieron.csv')\n",
    "norespondieron.to_csv('Inputs/df_norespondieron.csv')\n",
    "\n",
    "# Tasa de pobreza de hogares\n",
    "mean_pobres_hogar = respondieron['pobre_hogar_ponde'].sum() / respondieron['PONDIH'].sum() \n",
    "print(mean_pobres_hogar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontramos que la pobreza en nuestro calculo es solo del 26,33% para el Gran Buenos Aires y la Ciudad de Buenos Aires. Mientras que en el TP2, la pobreza era del 31,5% para el Gran Buenos Aires. Lo cual, aunque cercano, es menor a al dato publicado en el informe de indec el cual da un numero de 30.3%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parte II - Construcción de funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Función evalua_metodo(X_train, Y_trein, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalua_metodo(model, x_train, y_train, x_test, y_test):\n",
    "    '''\n",
    "    Esta función recibe como inputs un modelo ya definido y un dataset ya dividido entre X e Y para entrenamiento y test, \n",
    "    ajustando los datos al modelo brindado para generar los output consistentes en determinadas medidas de precisión: \n",
    "    accuracy, matriz de confusión y sus componentes (verdadero negativo, falspo positivo, falso negativo, verdadero positivo), área bajo la curva ROC, ECM y el Average Precision Score (AP). \n",
    "    '''\n",
    "    \n",
    "    modelofit = model.fit(x_train, y_train)\n",
    "    y_pred = modelofit.predict(x_test)\n",
    "    y_pred = np.where(y_pred > 0.5, 1, y_pred)\n",
    "    y_pred = np.where(y_pred <= 0.5, 0, y_pred)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    matriz_confusion = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp , fn, tp = confusion_matrix(y_test, y_pred).ravel() \n",
    "    ecm = mean_squared_error(y_test, y_pred)\n",
    "    ap = average_precision_score(y_test, y_pred)\n",
    "\n",
    "    return (accuracy, matriz_confusion, tn, fp, fn, tp, auc, ecm, ap,modelofit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Funcioón cross_validatio(model, k, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(model, k, x, y):\n",
    "    '''\n",
    "    Esta función toma como inputs un modelo ya configurado, el K para saber la cantidad de iteraciones a realizarse en \n",
    "    k-fold CV y los dataset con las variables (x,y). \n",
    "    Lo que hace entonces es parte al dataset en K particiones de entrenamiento y test, aplicándole a cada una la función\n",
    "    evalua_metodo. \n",
    "    El output está formado por diferentes métricas de precisión para cada una de las particiones analizadas:\n",
    "    es una colección del K, accuracy, ECM, AP y el modelo analizado.\n",
    "    '''\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=10)\n",
    "    resultados = pd.DataFrame(columns=[\"K\", \"accuracy\", \"ecm\", \"ap\", \"auc\", \"modelo\"]) \n",
    "    for i, (train_index, test_index) in enumerate(kf.split(x)):   \n",
    "        x_train, x_test = x.iloc[train_index], x.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        sc = StandardScaler()\n",
    "        X_train_transformed = pd.DataFrame(sc.fit_transform(x_train),index=x_train.index, columns=x_train.columns)\n",
    "        X_test_transformed = pd.DataFrame(sc.transform(x_test),index=x_test.index, columns=x_test.columns)\n",
    "        \n",
    "        accuracy, matriz_confusion, tn, fp, fn, tp, auc, ecm, ap, modelofit = evalua_metodo(model, X_train_transformed, y_train, X_test_transformed, y_test)\n",
    "        K = i+1\n",
    "        resultados = resultados._append({\"K\":i+1, \"accuracy\":accuracy, \"ecm\":ecm, \"ap\":ap, \"auc\": auc, \"modelo\":modelofit}, ignore_index=True)\n",
    "    #return resultados\n",
    "    return (K, accuracy, auc, ecm, ap, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Función evalua_config(landa, k, x, y, l1_ratio=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Toma cómo input una serie de valores de Lambda y el K para CV, y devuelve el lambda que minimiza el ECM por CV, usando elastic net para reg_logit\n",
    "    l1_ratio=1 se hace Lasso\n",
    "    l1_ratio=0 se hace Ridge\n",
    "    0 <l1_ratio< 1 combinación de ambos en elastic net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalua_config(modelo, lambdas, x, y,penalty1,k_range):\n",
    "    '''\n",
    "    Esta función tiene como objetivo iterar entre los distintos valores de lambda del modelo Logit para los valores de K\n",
    "    indicados, buscando obtener el valor de lambda que minimiza el ECM de la regresión. \n",
    "    El primer input es el modelo, el siguiente es la grilla con los valores de los lambda a iterar, \n",
    "    los dos siguientes son los dataset con las variables (x,y), finalizando con el penalty elegido para Logit \n",
    "    (l1 para LASSO, l2 para Ridge) y el valor de K. \n",
    "    \n",
    "    El output de la función es el valor que toma el ECM para el lambda que minimiza esta medida y dicho valor para este \n",
    "    hiperparámetro.\n",
    "    '''\n",
    "     # La idea acá es que tome los valores del modelo logit del punto 4 (parámetro \"modelo\"), donde definimos los parámetros que le metemos a cross_validation.     \n",
    "    ecms =  pd.DataFrame(columns=[\"alp\", \"ecm\", \"K\",\"penalty\"])\n",
    "    \n",
    "    if modelo == \"logit\":\n",
    "    \n",
    "        for i in lambdas:\n",
    "            for k11 in k_range:\n",
    "                C_alpha = 1/i\n",
    "                model = LogisticRegression(penalty = penalty1, C = C_alpha, max_iter=1000, solver=\"saga\")\n",
    "                k, accuracy, auc, ecm, ap, modelo = cross_validation(model,k11,x,y)\n",
    "                ecms = ecms._append({\"alp\":i, \"ecm\":ecm, \"K\": k,\"penalty\":penalty1}, ignore_index=True)\n",
    "                #ecms = ecms.astype({\"K\":int})\n",
    "    #return ecms\n",
    "    ecms_avg = ecms.groupby('alp').agg({'ecm':'mean'}).reset_index()\n",
    "    #return ecms_avg\n",
    "    min_ecm = np.Inf\n",
    "    alp_ecm = None\n",
    "    for index, row in ecms_avg.iterrows():\n",
    "            if row['ecm'] < min_ecm:\n",
    "                min_ecm = row['ecm']\n",
    "                alp_ecm = row['alp']\n",
    "    return (ecms,min_ecm,alp_ecm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Función evalua_multples_metodos(k_cv,k_knn, landa_max, x_train, x_test , y_train, y_test, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La idea acá adentro es definir los modelos que vamos a correr. Lo importante es esto de usar evalua_metodo para sacar el hiperparámetro de la logit. Los otros 2 supongo que podemos usar el que evaluemos por nuestra cuenta porque la consigna no pide específicamente sacarlo con la otra función.\n",
    "\n",
    "def evalua_multiples_metodos(modelos, x, y, parametros):\n",
    "    '''\n",
    "    El objetivo de esta función es poder generar una tabla con métricas descriptivas de la performance de diferentes modelos, \n",
    "    en base a los hiperparámetros determinados y/o la configuración brindada por el usuario al llamar la función. \n",
    "    De esta manera, el primer input es un diccionario con los modelos a correr: KNN para el modelo KNN, \n",
    "    LDA para el modelo de análisis de discriminante lineal y Logit para el modelo de regresión logística. \n",
    "    En segundo lugar, deben indicarse los dataset con las variables x e y para considerar en los modelos.\n",
    "    Finalmente, también debe indicarse un diccionario con el hiperparámetro alfa (lambda) de Logit a ser determinado \n",
    "    y la configuración de parámetros correspndiente, en el caso de KNN y LDA. \n",
    "    \n",
    "    Parámetros: \n",
    "    Para optimizar el lambda de la regularización, debe indicarse en el diccionario \"parametros\" los diferentes \n",
    "    valores de lambda a ser iterados, buscando obtenerse el que minimice el ECM. \n",
    "    También es necesario indicar la cantidad de particiones que desean probarse en CV a través del campo \n",
    "    \"k_range\" en el input \"parametros\", pudiendo ser un entero o un rango de valores. Lo mismo sucede con el campo \"neighbors\" para la cantidad de vecinos en KNN y \"components\" para los componentes de LDA.\n",
    "    Por otro lado, si se va a correr un modelo Logit, es necesario indicar el campo \"penalty\" de \"parameters\", \n",
    "    con una lista para las distintas penalidades a analizarse, siendo los valores l1 y l2 los correspondientes a LASSO y Ridge,\n",
    "    respectivamente.\n",
    "    \n",
    "    El output de la función, por su parte, es una tabla que indica el modelo analizado, \n",
    "    los valores de sus parámetros y las medidas de Accuracy, ECM, AP, AUC y la Matriz de Confusión.\n",
    "        \n",
    "    '''\n",
    "    tabla =  pd.DataFrame(columns=[\"Modelo\", \"Neighbors\", \"Components\", \"Penalty\", \"Alp_best\", \"Accuracy\", \"ECM\", \"AP\", \"AUC\", \"Verdadero 0\", \"Falso 1\", \"Falso 0\", \"Verdadero 1\"])\n",
    "    ecms_p = pd.DataFrame(columns=[\"alp\", \"ecm\", \"K\",\"penalty\"])\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=10)\n",
    "    for modelo in modelos:\n",
    "        if modelo == \"KNN\":\n",
    "            neighbors= parametros[\"neighbors\"]\n",
    "            accuracy, matriz_confusion, tn, fp, fn, tp, auc, ecm, ap, modelofit=evalua_metodo(KNeighborsClassifier(n_neighbors=neighbors),x_train,y_train,x_test,y_test)\n",
    "            tabla = tabla._append({\"Modelo\":modelo, \"Neighbors\": neighbors,\"Accuracy\":accuracy, \"AUC\":auc, \"ECM\":ecm, \"AP\":ap, \"Verdadero 0\": tn, \"Falso 1\": fp, \"Falso 0\": fn, \"Verdadero 1\": tp}, ignore_index=True)\n",
    "        elif modelo == \"LDA\":\n",
    "            components= parametros[\"components\"]\n",
    "            accuracy, matriz_confusion, tn, fp, fn, tp, auc, ecm, ap, modelofit = evalua_metodo(LinearDiscriminantAnalysis(n_components=components),x_train,y_train,x_test,y_test)\n",
    "            tabla = tabla._append({\"Modelo\":modelo, \"Components\": components,\"Accuracy\":accuracy, \"AUC\":auc,\"ECM\":ecm, \"AP\":ap, \"Verdadero 0\": tn, \"Falso 1\": fp, \"Falso 0\": fn, \"Verdadero 1\": tp}, ignore_index=True)\n",
    "        elif modelo == \"Logit\":\n",
    "            for p in parametros[\"penalty\"]:\n",
    "                ecms, min_ecm, alp_ecm = evalua_config(\"logit\",parametros[\"lambda\"],x,y,p, parametros[\"k_range\"])\n",
    "                sc = StandardScaler()\n",
    "                X_train_transformed = pd.DataFrame(sc.fit_transform(x_train),index=x_train.index, columns=x_train.columns)\n",
    "                X_test_transformed = pd.DataFrame(sc.transform(x_test),index=x_test.index, columns=x_test.columns)\n",
    "                accuracy, matriz_confusion, tn, fp, fn, tp, auc, ecm, ap, modelofit = evalua_metodo(LogisticRegression(penalty = p, C = 1/alp_ecm, max_iter=1000, solver=\"saga\"),x_train,y_train,x_test,y_test)\n",
    "                tabla = tabla._append({\"Modelo\":modelo, \"Penalty\": p,\"Alp_best\": alp_ecm,\"Accuracy\":accuracy, \"AUC\":auc, \"ECM\":ecm, \"AP\":ap, \"Verdadero 0\": tn, \"Falso 1\": fp, \"Falso 0\": fn, \"Verdadero 1\": tp}, ignore_index=True)\n",
    "                ecms_p = ecms_p._append(ecms)\n",
    "    tabla = tabla.fillna('')\n",
    "    return [tabla,ecms_p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parte III: Clasificación y regularización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de esta parte del trabajo es nuevamente intentar predecir si una persona es o no pobre utilizando datos distintos al ingreso, dado que muchos hogares son reacios a responder cuánto ganan. Esta vez lo haremos con la base unida de las preguntas de la encuesta individual y la encuesta de hogar. A su vez, incluiremos ejercicios de regularización y de validación cruzada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ejercicio 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos las variables relacionadas a ingresos y las columnas de las bases respondieron/no respondieron\n",
    "respondieron = respondieron.drop(['P21', 'DECOCUR',  'RDECOCUR', 'GDECOCUR', 'ADECOCUR', 'TOT_P12'], axis=1) # ingresos de la ocupación principal y de otras ocupaciones\n",
    "respondieron = respondieron.drop(['P47T', 'DECINDR',  'RDECINDR', 'GDECINDR', 'ADECINDR'], axis=1) # ingresos total individual\n",
    "respondieron = respondieron.drop(['V2_M', 'V3_M', 'V4_M', 'V5_M', 'V8_M', 'V9_M', 'V10_M', 'V11_M', 'V12_M', 'V18_M', 'V19_AM', 'V21_M', 'T_VI'], axis=1) # ingresos no laborales\n",
    "respondieron = respondieron.drop(['ITF', 'DECIFR', 'RDECIFR', 'GDECIFR',  'ADECIFR'], axis=1) # ingreso total familiar\n",
    "respondieron = respondieron.drop(['IPCF', 'DECCFR', 'RDECCFR', 'GDECCFR',  'ADECCFR'], axis=1) # ingreso per capita familiar\n",
    "respondieron = respondieron.drop(['adulto_equiv', 'ad_equiv_hogar', 'ingreso_necesario'], axis=1) # columnas adulto_equiv, ad_equiv_hogar e ingreso_necesario\n",
    "\n",
    "norespondieron = norespondieron.drop(['P21', 'DECOCUR',  'RDECOCUR', 'GDECOCUR', 'ADECOCUR', 'TOT_P12'], axis=1) # ingresos de la ocupación principal y de otras ocupaciones\n",
    "norespondieron = norespondieron.drop(['P47T', 'DECINDR',  'RDECINDR', 'GDECINDR',  'ADECINDR'], axis=1) # ingresos total individual\n",
    "norespondieron = norespondieron.drop(['V2_M', 'V3_M', 'V4_M', 'V5_M', 'V8_M', 'V9_M', 'V10_M', 'V11_M', 'V12_M', 'V18_M', 'V19_AM', 'V21_M', 'T_VI'], axis=1) # ingresos no laborales\n",
    "norespondieron = norespondieron.drop(['ITF', 'DECIFR', 'RDECIFR', 'GDECIFR',  'ADECIFR'], axis=1) # ingreso total familiar\n",
    "norespondieron = norespondieron.drop(['IPCF', 'DECCFR',  'RDECCFR', 'GDECCFR',  'ADECCFR'], axis=1) # ingreso per capita familiar\n",
    "norespondieron = norespondieron.drop(['adulto_equiv', 'ad_equiv_hogar'], axis=1) # columnas adulto_equiv, ad_equiv_hogar e ingreso_necesario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ejercicio 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = respondieron['pobre']\n",
    "# Nos quedamos con las variables de la base que nos sirven para hacer la regresion\n",
    "x = respondieron[['IV1', 'IV3', 'IV6', 'IV7', 'IV8', 'IV9', 'IV12_1', 'IV12_3', 'II7', 'II8', 'V1', 'V2', 'V5', 'CH09', 'CH10', 'CH11', 'CH12', 'CH06']]\n",
    "x = sm.add_constant(x) # Agregamos la columna de unos.\n",
    "x=x.astype('int')\n",
    "y=y.astype('int')\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state=101)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_22720\\3625693842.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  tabla = tabla._append({\"Modelo\":modelo, \"Neighbors\": neighbors,\"Accuracy\":accuracy, \"AUC\":auc, \"ECM\":ecm, \"AP\":ap, \"Verdadero 0\": tn, \"Falso 1\": fp, \"Falso 0\": fn, \"Verdadero 1\": tp}, ignore_index=True)\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_22720\\593790229.py:22: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  resultados = resultados._append({\"K\":i+1, \"accuracy\":accuracy, \"ecm\":ecm, \"ap\":ap, \"auc\": auc, \"modelo\":modelofit}, ignore_index=True)\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_22720\\572665800.py:22: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  ecms = ecms._append({\"alp\":i, \"ecm\":ecm, \"K\": k,\"penalty\":penalty1}, ignore_index=True)\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_22720\\593790229.py:22: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  resultados = resultados._append({\"K\":i+1, \"accuracy\":accuracy, \"ecm\":ecm, \"ap\":ap, \"auc\": auc, \"modelo\":modelofit}, ignore_index=True)\n",
      "c:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_22720\\3625693842.py:45: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  tabla = tabla._append({\"Modelo\":modelo, \"Penalty\": p,\"Alp_best\": alp_ecm,\"Accuracy\":accuracy, \"AUC\":auc, \"ECM\":ecm, \"AP\":ap, \"Verdadero 0\": tn, \"Falso 1\": fp, \"Falso 0\": fn, \"Verdadero 1\": tp}, ignore_index=True)\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_22720\\3625693842.py:46: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  ecms_p = ecms_p._append(ecms)\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_22720\\593790229.py:22: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  resultados = resultados._append({\"K\":i+1, \"accuracy\":accuracy, \"ecm\":ecm, \"ap\":ap, \"auc\": auc, \"modelo\":modelofit}, ignore_index=True)\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_22720\\572665800.py:22: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  ecms = ecms._append({\"alp\":i, \"ecm\":ecm, \"K\": k,\"penalty\":penalty1}, ignore_index=True)\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_22720\\593790229.py:22: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  resultados = resultados._append({\"K\":i+1, \"accuracy\":accuracy, \"ecm\":ecm, \"ap\":ap, \"auc\": auc, \"modelo\":modelofit}, ignore_index=True)\n",
      "c:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modelo</th>\n",
       "      <th>Neighbors</th>\n",
       "      <th>Components</th>\n",
       "      <th>Penalty</th>\n",
       "      <th>Alp_best</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>ECM</th>\n",
       "      <th>AP</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Verdadero 0</th>\n",
       "      <th>Falso 1</th>\n",
       "      <th>Falso 0</th>\n",
       "      <th>Verdadero 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN</td>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.754658</td>\n",
       "      <td>0.245342</td>\n",
       "      <td>0.321436</td>\n",
       "      <td>0.582520</td>\n",
       "      <td>224</td>\n",
       "      <td>16</td>\n",
       "      <td>63</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LDA</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.804348</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.430932</td>\n",
       "      <td>0.664024</td>\n",
       "      <td>228</td>\n",
       "      <td>12</td>\n",
       "      <td>51</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logit</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>l1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.807453</td>\n",
       "      <td>0.192547</td>\n",
       "      <td>0.434494</td>\n",
       "      <td>0.658079</td>\n",
       "      <td>231</td>\n",
       "      <td>9</td>\n",
       "      <td>53</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logit</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>l2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.804348</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.427573</td>\n",
       "      <td>0.655996</td>\n",
       "      <td>230</td>\n",
       "      <td>10</td>\n",
       "      <td>53</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Modelo Neighbors Components Penalty Alp_best  Accuracy       ECM        AP  \\\n",
       "0    KNN         5                              0.754658  0.245342  0.321436   \n",
       "1    LDA                    1                   0.804348  0.195652  0.430932   \n",
       "2  Logit                           l1      2.0  0.807453  0.192547  0.434494   \n",
       "3  Logit                           l2      2.0  0.804348  0.195652  0.427573   \n",
       "\n",
       "        AUC  Verdadero 0  Falso 1  Falso 0  Verdadero 1  \n",
       "0  0.582520          224       16       63           19  \n",
       "1  0.664024          228       12       51           31  \n",
       "2  0.658079          231        9       53           29  \n",
       "3  0.655996          230       10       53           29  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelos = [\"KNN\",\"LDA\",\"Logit\"]\n",
    "\n",
    "parametros = {\"k_range\": range(2,3), \"neighbors\":5, \"components\":1, \"penalty\": [\"l1\",\"l2\"], \"lambda\": [2,3]} #Penalty de Lasso, la l2 es Ridge. \n",
    "\n",
    "#alphas = list(10**np.linspace(6,-2,50)*0.5)\n",
    "\n",
    "lasso_2 = evalua_multiples_metodos(modelos,x,y,parametros)\n",
    "                                                 \n",
    "lasso_2[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

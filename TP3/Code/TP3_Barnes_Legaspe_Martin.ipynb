{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parte I - Análisis de la base de hogares y cálculo de pobreza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Analisis exploratorio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comezamos pensando, solo intuitvamente cuales a priori son las variables que podrian ser utilies para predecir la pobreza y perfeccionar las estimaciones el TP2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dentro de la varibles en la encuesta de hogares consideramos relevantes las caracterisicas de la vivienda:\n",
    "* El tipo de vivienda (IV1)\n",
    "* Cantidad de habitaciones (IV2)\n",
    "* Material del techo (V4)\n",
    "* Revestimiento del techo (IV5)\n",
    "* Tipo de conexion a agua (IV6 y IV7)\n",
    "* Tener baño (IV8), tipo de baño (IV9) y equipamiento (IV10)\n",
    "* Tipo de desague (IV11)\n",
    "* Zona de ubicacion: cercano a basural (IV12_1), zona inundable (IV12_2) y/o villa de emergecia (IV12_3)\n",
    "\n",
    "\n",
    "Las caractersiticas habitacionales:\n",
    "* Cantidad de ambientes (II1)\n",
    "* Cantidad de dormitorios (II2)\n",
    "* Habitaccion para trabajo (II3), exlusivamente (II6)\n",
    "* Tienen: cuarto de cocina (II4_1), lavadero (II4_2), garage (II4_3), cuantos de estos se usan para dormir (II5).\n",
    "* Regimen de tenencia (II7)\n",
    "* Combustible para cocinar (II8)\n",
    "* Propiedad del baño (II9)\n",
    "\n",
    "Variables de tipo de ingresos: V1 a 18. Y variables de trabajo infantil: V19_A y V19_B\n",
    "\n",
    "Integrantes del hogar: total (IX_Tot) y menores de 10 (IX_men10).\n",
    "\n",
    "No icluimos las que son mediciones de ingresos al igual que en el TP2.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Continuamos descargando la base de hogares y cargandola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos las librerias necesarias\n",
    "import os  \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt  # Para matriz de correlaciones\n",
    "import statsmodels.api as sm     \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression, Lasso, LassoCV, Ridge, RidgeCV\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV, RepeatedKFold\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix, accuracy_score, roc_curve, RocCurveDisplay, average_precision_score,roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.special import expit\n",
    "\n",
    "\n",
    "\n",
    "# Definimos el directorio\n",
    "# os.chdir('C:/Users/rodri/OneDrive/Escritorio/Maestría/Big Data/BigData/TP3')\n",
    "os.chdir('C:/Users/Usuario/Desktop/MAESTRIA/Big Data/TPs/BigData/TP3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos los datos, menteniendo solo las observaciones para el Gran Buenis Aires y la Ciudad de Buenos Aires:\n",
    "hogares = pd.read_excel(\"inputs/usu_hogar_T123.xlsx\")\n",
    "hogares = hogares[(hogares['AGLOMERADO']==32) | (hogares['AGLOMERADO']==33)]\n",
    "individual = pd.read_excel(\"inputs/usu_individual_T123.xlsx\")\n",
    "individual = individual[(individual['AGLOMERADO']==32) | (individual['AGLOMERADO']==33)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Unimos la tablas de Individuos y hogares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chequeamos que columnas estan duplicadas para que no se nos duplique en el merge\n",
    "columnas_duplicadas = set(hogares.columns).intersection(set(individual.columns))\n",
    "# Removemos CODUSU y NRO_HOGAR de la lista, para mantenerlas como ids del merge\n",
    "columnas_duplicadas.remove(\"CODUSU\")\n",
    "columnas_duplicadas.remove(\"NRO_HOGAR\")\n",
    "# Eliminamos los duplicados de la base hogar\n",
    "hogares.drop(columnas_duplicadas, axis=1, inplace=True)\n",
    "\n",
    "# Hacemos el left join de los hogares con los individuos\n",
    "df = pd.merge(individual,hogares, on=[\"CODUSU\", \"NRO_HOGAR\"], how=\"left\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Herramientas de limpieza de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la limpieza utilizaremos las herramientas que nos ofrece el modulo `pandas`, para el manejo de DataFrames. A continuación, explicaré las funciones que se están utilizando:\n",
    "\n",
    "   - la función `describe()` proporciona información resumida sobre la distribución de valores en esa columna. Incluyendo la media, minimo, desviación estánda, cuartiles, etc.\n",
    "\n",
    "   - `isnull()` es un método de los DataFrames de pandas que devuelve una matriz booleana indicando las ubicaciones de los valores faltantes en el DataFrame.\n",
    "\n",
    "   - `dropna()` es un método de los DataFrames de pandas que elimina las filas o columnas con valores faltantes.\n",
    "\n",
    "   - La función `fillna()` se utiliza para rellenar los valores faltantes en la columna.\n",
    "\n",
    "   - La función `drop()` con `axis=1`, elimina una columna inidicada.\n",
    "\n",
    "   -  La función `duplicated()` es un método de pandas que devuelve una Serie de valores booleanos que indica si cada fila del DataFrame es una duplicada de una fila anterior. Con `sum()` contamos estos booleanos generado de modo de obtener la cantidad de duplicados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Limpieza de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos un cheaqueo rapido de los datos, generando una tabla de estadisticas descriptivas\n",
    "descripcion = df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chequeamos que no haya duplicados\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chequeamoss el porcentaje de missing values\n",
    "percent_missing = df.isnull().sum() * 100 / len(df)\n",
    "missing_value_eph = pd.DataFrame({'Columna': df.columns,\n",
    "                                 'Porcentaje de NAs': percent_missing})\n",
    "missing_value_eph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Eliminimamos la varaible si esta tiene mas de 90% de percent_missing\n",
    "df = df.dropna(thresh=len(df)*0.9, axis=1)\n",
    "# Chequeamos nuevamente los missing\n",
    "percent_missing = df.isnull().sum() * 100 / len(df)\n",
    "missing_value_eph = pd.DataFrame({'Columna': df.columns,\n",
    "                                 'Porcentaje de NAs': percent_missing})\n",
    "# Remplazamos los missings en CH08 (cobertura medica) por 9 = Ns/Nc\n",
    "df['CH08'] = df['CH08'].fillna(9)\n",
    "# Reempalazamos el 9 por 0\n",
    "df['CH08'] = df['CH08'].replace(9,0)\n",
    "\n",
    "# Consideramos que el que sea missing brinda informacion relevante, por lo que lo dejamos como una categoria mas\n",
    "\n",
    "# De este modo no tenemos mas NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cheaqueamos que no existan ingresos negativos\n",
    "df['ITF'].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cheqqueamos que no haya edades negativas\n",
    "df['CH06'].describe()\n",
    "\n",
    "# eliminamos observaciones con datos NS/NC en la variable para cantidad de habitaciones \n",
    "df = df[(df['IV2'] < 99)]\n",
    "\n",
    "# Remplazamos las que son NS/NR por 0\n",
    "df['CH11'] = df['CH11'].replace(9,0)\n",
    "df['CH13'] = df['CH13'].replace(3,0)\n",
    "df['CH15'] = df['CH15'].replace(9,0)\n",
    "df['CH16'] = df['CH16'].replace(9,0)\n",
    "df['NIVEL_ED'] = df['NIVEL_ED'].replace(9,0)\n",
    "df['CAT_OCUP'] = df['CAT_OCUP'].replace(9,0)\n",
    "# Al hacer esto mo perdemos la observacion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora vamos a crear las variables dummy para las categoricas\n",
    "for col in ['IV1', 'IV3', 'IV6', 'IV7', 'IV8', 'IV9', 'IV12_1', 'IV12_3', 'II7', 'II8', 'V1', 'V2', 'V5', 'DECCFR', 'CH09', 'CH10', 'CH11', 'CH12']:\n",
    "    df[col] = df[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hacemos un ultimo check de missings\n",
    "percent_missing1 = df.isnull().sum() * 100 / len(df)\n",
    "missing_value_eph1 = pd.DataFrame({'Columna': df.columns,\n",
    "                                 'Porcentaje de NAs': percent_missing})\n",
    "missing_value_eph1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Estadisticas descriptivas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación presentamos las estadisticas descriptivas, de cinco variable de la encuesta de hogares que consideramos relevantes: Total  de integrantes del hogar(IX_Tot), integrantes menores de 10 años (IX_men10), cantidad de habitaciones (IV2), vivienda en zona inundable (IV12_2), y vivienda en villa de emergecia (IV12_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Varaibles de interes\n",
    "variables_interes = df[['IX_TOT', 'IX_MEN10', 'IV2', 'IV12_2', 'IV12_3']]\n",
    "\n",
    "variables_interes.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que para los 2742 hogare en la emcuesta, en promedio  hay 3 integrantes por hogar, y que aproximadamente solo uno de cada tres hogares tienen un nicño menor a 10 años. En promedio hay 3 habitaciones por hogar, y solo el 2% de los hogares se encuentran en zonas inundables o en villas de emergencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Calculo de Adulto Equivalente y Requerimentos del Hogar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargaos la tabla de equivalencia de adultos\n",
    "adulto_equiv_data = pd.read_excel(\"inputs/tabla_adulto_equiv.xlsx\")\n",
    "\n",
    "# melt adulto_equiv_data Muejeres y Hombres\n",
    "adulto_equiv_data = pd.melt(adulto_equiv_data, id_vars=['Edad'], value_name= 'adulto_equiv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Change values of variable column to 1 if Hom and 2 if Muj\n",
    "adulto_equiv_data['variable'] = np.where(adulto_equiv_data['variable']=='Mujeres', 2, 1)\n",
    "# Rename variable column to Sexo\n",
    "adulto_equiv_data = adulto_equiv_data.rename(columns={'variable': 'Sexo'})\n",
    "\n",
    "#Función que lee los valores de edad en números CH06 y me lo impacta en la categoría etarea correspondiende de la nueva tabla.\n",
    "def rango_edad(edad):\n",
    "    if edad < 0:\n",
    "        rangoetareo = \"Menor de 1 años\"\n",
    "    elif edad > 0 and edad < 18:\n",
    "        rangoetareo = str(edad)+\" años\"\n",
    "    elif 17 < edad and edad < 30:\n",
    "        rangoetareo = \"18 a 29 años\"\n",
    "    elif 29 < edad and edad < 46:\n",
    "        rangoetareo = \"30 a 45 años\"\n",
    "    elif 45 < edad and edad < 61:\n",
    "        rangoetareo = \"46 a 60 años\"\n",
    "    elif 60 < edad and edad < 76:\n",
    "        rangoetareo = \"61 a 75 años\"\n",
    "    elif edad > 75:\n",
    "        rangoetareo = \"más de 75 años\"\n",
    "        \n",
    "    else:\n",
    "        rangoetareo = 'NaN'\n",
    "    return rangoetareo\n",
    "\n",
    "#Aplico la función a mi tabla para crear la columna deseada\n",
    "df['rango_etareo'] = df['CH06'].apply(rango_edad)\n",
    "\n",
    "\n",
    "#Renombro la columna \"edad\", igual que la de la otra tabla\n",
    "df = df.rename(columns={'rango_etareo': 'Edad'})\n",
    "df = df.rename(columns={'CH04': 'Sexo'})\n",
    "\n",
    "# Mejoramos la funcion de match en relacion al TP2 donde los match para los menores de 1 año no estaban bien definidos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hacemos el merge de las dos tablas\n",
    "df = df.merge(adulto_equiv_data, on=['Sexo','Edad'], how='left')\n",
    "#Sumo para las personas de un mismo hogary loguardo como ad_equiv_hogar\n",
    "df['ad_equiv_hogar'] = df.groupby('CODUSU')['adulto_equiv'].transform('sum')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Particiones segun respuesta y ingreso necesario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punto 1.3: Particionamos la muestra segun respondieron o no sobre los ingresos:\n",
    "respondieron = df[df['ITF'] > 0]\n",
    "respondieron = respondieron.reset_index(drop=True)\n",
    "norespondieron = df[df['ITF'] <= 0]\n",
    "norespondieron = norespondieron.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Punto 1.4: Agregar columna que indica el ingreso necesario del hogar para no ser pobre\n",
    "respondieron['ingreso_necesario'] = respondieron['ad_equiv_hogar'] * 53371.05\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Generamos una indicadora de persona pobre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Punto 1.4: Columna que indica si una persona es pobre según el ingreso de su hogar\n",
    "respondieron.loc[:, 'pobre'] = (respondieron['ingreso_necesario'] > respondieron['ITF']).astype(int)\n",
    "mean_pobres = respondieron['pobre'].mean()\n",
    "print(mean_pobres)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Calculo de pobreza por hogares (muestra expandida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calaculamos la tasa de pobreza de hogares, expondiendo la muestra con el ponderador de hogares, PONDIH.\n",
    "# Columna indicadora de pobreza de hogares\n",
    "respondieron.loc[:, 'pobre_hogar'] = (respondieron['ingreso_necesario'] > respondieron['ITF']).astype(int)\n",
    "\n",
    "# Tasa de pobreza de hogares (sin expandir)\n",
    "mean_pobres_hogar_temp = respondieron['pobre_hogar'].mean()\n",
    "print(mean_pobres_hogar_temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ponderar los hogares pobres\n",
    "respondieron['pobre_hogar_ponde'] = respondieron['PONDIH'] * respondieron['pobre_hogar']\n",
    "\n",
    "\n",
    "# Tasa de pobreza de hogares\n",
    "mean_pobres_hogar = respondieron['pobre_hogar_ponde'].sum() / respondieron['PONDIH'].sum() \n",
    "print(mean_pobres_hogar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontramos que la pobreza en nuestro calculo es solo del 26,33% para el Gran Buenos Aires y la Ciudad de Buenos Aires. Mientras que en el TP2, la pobreza era del 31,5% para el Gran Buenos Aires. Lo cual, aunque cercano, es menor a al dato publicado en el informe de indec el cual da un numero de 30.3%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parte II - Construcción de funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Función evalua_metodo(X_train, Y_trein, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalua_metodo(model, x_train, y_train, x_test, y_test):\n",
    "    '''\n",
    "    Esta función recibe como inputs un modelo ya definido y un dataset ya dividido entre X e Y para entrenamiento y test, \n",
    "    ajustando los datos al modelo brindado para generar los output consistentes en determinadas medidas de precisión: \n",
    "    accuracy, matriz de confusión y sus componentes (verdadero negativo, falspo positivo, falso negativo, verdadero positivo), área bajo la curva ROC, ECM y el Average Precision Score (AP). \n",
    "    '''\n",
    "    \n",
    "    modelofit = model.fit(x_train, y_train)\n",
    "    y_pred = modelofit.predict(x_test)\n",
    "    y_pred = np.where(y_pred > 0.5, 1, y_pred)\n",
    "    y_pred = np.where(y_pred <= 0.5, 0, y_pred)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    matriz_confusion = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp , fn, tp = confusion_matrix(y_test, y_pred).ravel() \n",
    "    ecm = mean_squared_error(y_test, y_pred)\n",
    "    ap = average_precision_score(y_test, y_pred)\n",
    "\n",
    "    return (accuracy, matriz_confusion, tn, fp, fn, tp, auc, ecm, ap,modelofit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Funcioón cross_validatio(model, k, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(model, k, x, y):\n",
    "    '''\n",
    "    Esta función toma como inputs un modelo ya configurado, el K para saber la cantidad de iteraciones a realizarse en \n",
    "    k-fold CV y los dataset con las variables (x,y). \n",
    "    Lo que hace entonces es parte al dataset en K particiones de entrenamiento y test, aplicándole a cada una la función\n",
    "    evalua_metodo. \n",
    "    El output está formado por diferentes métricas de precisión para cada una de las particiones analizadas:\n",
    "    es una colección del K, accuracy, ECM, AP y el modelo analizado.\n",
    "    '''\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=10)\n",
    "    resultados = pd.DataFrame(columns=[\"K\", \"accuracy\", \"ecm\", \"ap\", \"auc\", \"modelo\"]) \n",
    "    for i, (train_index, test_index) in enumerate(kf.split(x)):   \n",
    "        x_train, x_test = x.iloc[train_index], x.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        sc = StandardScaler()\n",
    "        X_train_transformed = pd.DataFrame(sc.fit_transform(x_train),index=x_train.index, columns=x_train.columns)\n",
    "        X_test_transformed = pd.DataFrame(sc.transform(x_test),index=x_test.index, columns=x_test.columns)\n",
    "        \n",
    "        accuracy, matriz_confusion, tn, fp, fn, tp, auc, ecm, ap, modelofit = evalua_metodo(model, X_train_transformed, y_train, X_test_transformed, y_test)\n",
    "        K = i+1\n",
    "        resultados = resultados._append({\"K\":i+1, \"accuracy\":accuracy, \"ecm\":ecm, \"ap\":ap, \"auc\": auc, \"modelo\":modelofit}, ignore_index=True)\n",
    "    #return resultados\n",
    "    return (K, accuracy, auc, ecm, ap, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Función evalua_config(landa, k, x, y, l1_ratio=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalua_config(modelo, lambdas, x, y,penalty1,k_range):\n",
    "    '''\n",
    "    Esta función tiene como objetivo iterar entre los distintos valores de lambda del modelo Logit para los valores de K\n",
    "    indicados, buscando obtener el valor de lambda que minimiza el ECM de la regresión. \n",
    "    El primer input es el modelo, el siguiente es la grilla con los valores de los lambda a iterar, \n",
    "    los dos siguientes son los dataset con las variables (x,y), finalizando con el penalty elegido para Logit \n",
    "    (l1 para LASSO, l2 para Ridge) y el valor de K. \n",
    "    \n",
    "    El output de la función es el valor que toma el ECM para el lambda que minimiza esta medida y dicho valor para este \n",
    "    hiperparámetro.\n",
    "    '''\n",
    "     # La idea acá es que tome los valores del modelo logit del punto 4 (parámetro \"modelo\"), donde definimos los parámetros que le metemos a cross_validation.     \n",
    "    ecms =  pd.DataFrame(columns=[\"alp\", \"ecm\", \"K\",\"penalty\"])\n",
    "    \n",
    "    if modelo == \"logit\":\n",
    "    \n",
    "        for i in lambdas:\n",
    "            for k11 in k_range:\n",
    "                C_alpha = 1/i\n",
    "                model = LogisticRegression(penalty = penalty1, C = C_alpha, max_iter=1000, solver=\"saga\")\n",
    "                k, accuracy, auc, ecm, ap, modelo = cross_validation(model,k11,x,y)\n",
    "                ecms = ecms._append({\"alp\":i, \"ecm\":ecm, \"K\": k,\"penalty\":penalty1}, ignore_index=True)\n",
    "                #ecms = ecms.astype({\"K\":int})\n",
    "    #return ecms\n",
    "    ecms_avg = ecms.groupby('alp').agg({'ecm':'mean'}).reset_index()\n",
    "    #return ecms_avg\n",
    "    min_ecm = np.Inf\n",
    "    alp_ecm = None\n",
    "    for index, row in ecms_avg.iterrows():\n",
    "            if row['ecm'] < min_ecm:\n",
    "                min_ecm = row['ecm']\n",
    "                alp_ecm = row['alp']\n",
    "    return (ecms,min_ecm,alp_ecm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Función evalua_multples_metodos(k_cv,k_knn, landa_max, x_train, x_test , y_train, y_test, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La idea acá adentro es definir los modelos que vamos a correr. Lo importante es esto de usar evalua_metodo para sacar el hiperparámetro de la logit. Los otros 2 supongo que podemos usar el que evaluemos por nuestra cuenta porque la consigna no pide específicamente sacarlo con la otra función.\n",
    "\n",
    "def evalua_multiples_metodos(modelos, x, y, parametros):\n",
    "    '''\n",
    "    El objetivo de esta función es poder generar una tabla con métricas descriptivas de la performance de diferentes modelos, \n",
    "    en base a los hiperparámetros determinados y/o la configuración brindada por el usuario al llamar la función. \n",
    "    De esta manera, el primer input es un diccionario con los modelos a correr: KNN para el modelo KNN, \n",
    "    LDA para el modelo de análisis de discriminante lineal y Logit para el modelo de regresión logística. \n",
    "    En segundo lugar, deben indicarse los dataset con las variables x e y para considerar en los modelos.\n",
    "    Finalmente, también debe indicarse un diccionario con el hiperparámetro alfa (lambda) de Logit a ser determinado \n",
    "    y la configuración de parámetros correspndiente, en el caso de KNN y LDA. \n",
    "    \n",
    "    Parámetros: \n",
    "    Para optimizar el lambda de la regularización, debe indicarse en el diccionario \"parametros\" los diferentes \n",
    "    valores de lambda a ser iterados, buscando obtenerse el que minimice el ECM. \n",
    "    También es necesario indicar la cantidad de particiones que desean probarse en CV a través del campo \n",
    "    \"k_range\" en el input \"parametros\", pudiendo ser un entero o un rango de valores. Lo mismo sucede con el campo \"neighbors\" para la cantidad de vecinos en KNN y \"components\" para los componentes de LDA.\n",
    "    Por otro lado, si se va a correr un modelo Logit, es necesario indicar el campo \"penalty\" de \"parameters\", \n",
    "    con una lista para las distintas penalidades a analizarse, siendo los valores l1 y l2 los correspondientes a LASSO y Ridge,\n",
    "    respectivamente.\n",
    "    \n",
    "    El output de la función, por su parte, es una tabla que indica el modelo analizado, \n",
    "    los valores de sus parámetros y las medidas de Accuracy, ECM, AP, AUC y la Matriz de Confusión.\n",
    "        \n",
    "    '''\n",
    "    tabla =  pd.DataFrame(columns=[\"Modelo\", \"Neighbors\", \"Components\", \"Penalty\", \"Alp_best\", \"Accuracy\", \"ECM\", \"AP\", \"AUC\", \"Verdadero 0\", \"Falso 1\", \"Falso 0\", \"Verdadero 1\"])\n",
    "    ecms_p = pd.DataFrame(columns=[\"alp\", \"ecm\", \"K\",\"penalty\"])\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=10)\n",
    "    for modelo in modelos:\n",
    "        if modelo == \"KNN\":\n",
    "            neighbors= parametros[\"neighbors\"]\n",
    "            accuracy, matriz_confusion, tn, fp, fn, tp, auc, ecm, ap, modelofit=evalua_metodo(KNeighborsClassifier(n_neighbors=neighbors),x_train,y_train,x_test,y_test)\n",
    "            tabla = tabla._append({\"Modelo\":modelo, \"Neighbors\": neighbors,\"Accuracy\":accuracy, \"AUC\":auc, \"ECM\":ecm, \"AP\":ap, \"Verdadero 0\": tn, \"Falso 1\": fp, \"Falso 0\": fn, \"Verdadero 1\": tp}, ignore_index=True)\n",
    "        elif modelo == \"LDA\":\n",
    "            components= parametros[\"components\"]\n",
    "            accuracy, matriz_confusion, tn, fp, fn, tp, auc, ecm, ap, modelofit = evalua_metodo(LinearDiscriminantAnalysis(n_components=components),x_train,y_train,x_test,y_test)\n",
    "            tabla = tabla._append({\"Modelo\":modelo, \"Components\": components,\"Accuracy\":accuracy, \"AUC\":auc,\"ECM\":ecm, \"AP\":ap, \"Verdadero 0\": tn, \"Falso 1\": fp, \"Falso 0\": fn, \"Verdadero 1\": tp}, ignore_index=True)\n",
    "        elif modelo == \"Logit\":\n",
    "            for p in parametros[\"penalty\"]:\n",
    "                ecms, min_ecm, alp_ecm = evalua_config(\"logit\",parametros[\"lambda\"],x,y,p, parametros[\"k_range\"])\n",
    "                sc = StandardScaler()\n",
    "                X_train_transformed = pd.DataFrame(sc.fit_transform(x_train),index=x_train.index, columns=x_train.columns)\n",
    "                X_test_transformed = pd.DataFrame(sc.transform(x_test),index=x_test.index, columns=x_test.columns)\n",
    "                accuracy, matriz_confusion, tn, fp, fn, tp, auc, ecm, ap, modelofit = evalua_metodo(LogisticRegression(penalty = p, C = 1/alp_ecm, max_iter=1000, solver=\"saga\"),x_train,y_train,x_test,y_test)\n",
    "                tabla = tabla._append({\"Modelo\":modelo, \"Penalty\": p,\"Alp_best\": alp_ecm,\"Accuracy\":accuracy, \"AUC\":auc, \"ECM\":ecm, \"AP\":ap, \"Verdadero 0\": tn, \"Falso 1\": fp, \"Falso 0\": fn, \"Verdadero 1\": tp}, ignore_index=True)\n",
    "                ecms_p = ecms_p._append(ecms)\n",
    "    tabla = tabla.fillna('')\n",
    "    return [tabla,ecms_p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parte III: Clasificación y regularización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de esta parte del trabajo es nuevamente intentar predecir si una persona es o no pobre utilizando datos distintos al ingreso, dado que muchos hogares son reacios a responder cuánto ganan. Esta vez lo haremos con la base unida de las preguntas de la encuesta individual y la encuesta de hogar. A su vez, incluiremos ejercicios de regularización y de validación cruzada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ejercicio 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos las variables relacionadas a ingresos y las columnas de las bases respondieron/no respondieron\n",
    "respondieron = respondieron.drop(['P21', 'DECOCUR',  'RDECOCUR', 'GDECOCUR', 'ADECOCUR', 'TOT_P12'], axis=1) # ingresos de la ocupación principal y de otras ocupaciones\n",
    "respondieron = respondieron.drop(['P47T', 'DECINDR',  'RDECINDR', 'GDECINDR', 'ADECINDR'], axis=1) # ingresos total individual\n",
    "respondieron = respondieron.drop(['V2_M', 'V3_M', 'V4_M', 'V5_M', 'V8_M', 'V9_M', 'V10_M', 'V11_M', 'V12_M', 'V18_M', 'V19_AM', 'V21_M', 'T_VI'], axis=1) # ingresos no laborales\n",
    "respondieron = respondieron.drop(['ITF', 'DECIFR', 'RDECIFR', 'GDECIFR',  'ADECIFR'], axis=1) # ingreso total familiar\n",
    "respondieron = respondieron.drop(['IPCF', 'DECCFR', 'RDECCFR', 'GDECCFR',  'ADECCFR'], axis=1) # ingreso per capita familiar\n",
    "respondieron = respondieron.drop(['adulto_equiv', 'ad_equiv_hogar', 'ingreso_necesario'], axis=1) # columnas adulto_equiv, ad_equiv_hogar e ingreso_necesario\n",
    "# Eliminamos variable de idice y creadas a fines de la primera parte:\n",
    "respondieron = respondieron.drop(['CODUSU','ANO4','TRIMESTRE','NRO_HOGAR','COMPONENTE','CH05','MAS_500',\"REGION\",\"PONDERA\",'PONDIIO','PONDII','PONDIH','REALIZADA','Edad','pobre_hogar_ponde'], axis=1) \n",
    "\n",
    "\n",
    "\n",
    "norespondieron = norespondieron.drop(['P21', 'DECOCUR',  'RDECOCUR', 'GDECOCUR', 'ADECOCUR', 'TOT_P12'], axis=1) # ingresos de la ocupación principal y de otras ocupaciones\n",
    "norespondieron = norespondieron.drop(['P47T', 'DECINDR',  'RDECINDR', 'GDECINDR',  'ADECINDR'], axis=1) # ingresos total individual\n",
    "norespondieron = norespondieron.drop(['V2_M', 'V3_M', 'V4_M', 'V5_M', 'V8_M', 'V9_M', 'V10_M', 'V11_M', 'V12_M', 'V18_M', 'V19_AM', 'V21_M', 'T_VI'], axis=1) # ingresos no laborales\n",
    "norespondieron = norespondieron.drop(['ITF', 'DECIFR', 'RDECIFR', 'GDECIFR',  'ADECIFR'], axis=1) # ingreso total familiar\n",
    "norespondieron = norespondieron.drop(['IPCF', 'DECCFR',  'RDECCFR', 'GDECCFR',  'ADECCFR'], axis=1) # ingreso per capita familiar\n",
    "norespondieron = norespondieron.drop(['adulto_equiv', 'ad_equiv_hogar'], axis=1) # columnas adulto_equiv, ad_equiv_hogar e ingreso_necesario\n",
    "# Eliminamos variable de idice y creadas a fines de la primera parte:\n",
    "norespondieron = norespondieron.drop(['CODUSU','ANO4','TRIMESTRE','NRO_HOGAR','COMPONENTE','CH05','MAS_500',\"REGION\",\"PONDERA\",'PONDIIO','PONDII','PONDIH','REALIZADA','Edad'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ejercicio 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = respondieron['pobre']\n",
    "# Nos quedamos con las variables de la base que nos sirven para hacer la regresion\n",
    "x = respondieron.drop(['pobre','pobre_hogar'], axis=1)\n",
    "\n",
    "x = sm.add_constant(x) # Agregamos la columna de unos.\n",
    "x=x.astype('int')\n",
    "y=y.astype('int')\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state=101)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelos = [\"KNN\",\"LDA\",\"Logit\"]\n",
    "\n",
    "parametros = {\"k_range\": range(2,3), \"neighbors\":5, \"components\":1, \"penalty\": [\"l1\",\"l2\"], \"lambda\": [2,3]} #Penalty de Lasso, la l2 es Ridge. \n",
    "\n",
    "#alphas = list(10**np.linspace(6,-2,50)*0.5)\n",
    "\n",
    "lasso_2 = evalua_multiples_metodos(modelos,x,y,parametros)\n",
    "                                                 \n",
    "lasso_2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_2[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ejercicio 3 \n",
    "\n",
    "Al momento de elegir $\\lambda$ por validación cruzada debemos partir la base de datos en K particiones, cuanto más grande sea K, mayor será la cantidad de datos que se usarán para entrenar al modelo y menor la cantidad utilizada para testearlo. Este proceso de K- Fold-Cross-Validation lo que hace es obtener la configuración del parámetro que minimice el error de pronostico del modelo y nos quedaríamos con el que tenga el mínimo error de pronóstico por cross validation.\n",
    "\n",
    "En palabras más sencillas, dado un potencial valor de $\\lambda$, este algoritmo se encarga de estimar K veces los modelos examinados sobre K submuestras de los datos de entrenamiento, dejando en cada caso una porción de los datos disponible para el testeo del modelo. Se almancenan las estimaciones de ECM  (o cualquier otra métrica de capacidad predictiva- de cada iteración), y finalmente se promedia. Después de esto, se repite el procedimiento para todos los valores de $\\lambda$ que se quiera probar y el $\\lambda$ elegido es aquel de menor ECM promedio. \n",
    "\n",
    "\n",
    "\n",
    "Notar que en ningún momento el algoritmo utiliza a la base de test. Esto aumentaría el riesgo de overfitting. La idea es que la base de testing quede reservada para decirnos que tan bueno es nuestro modelo final en datos que nunca se utilizaron para su estimación. Esto nos permite de ver cómo se comportarían los modelos examinados al agregar data nueva, como pasaría en una situación real. En cambio, si estimáramos el lambda ya usando la base de test, el parametro sería elegido para reducir el error en esta misma base, lo que sería tautologico y no sería informativo sobre cuan bien esta configuración predeciría usando nueva data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 4\n",
    "\n",
    "El problema de usar un K muy pequeño es que se minimiza la base de test es decir, si uso un K=2 usamos 50% de los datos para train y 50% de los datos para test. Esto provocaría que el modelo se estime menos precisamente. \n",
    "Por otro lado, si usamos un K muy grande, maximizariamos la base de train pero con un costo computacional alto. Un problema que puede surgir es que la estimación del error en la base de test es más sensible a valores particulares como por ejemplo outliers. \n",
    "\n",
    "Por último, si elegimos un K=n, el modelo se estima n veces usando n-1 observaciones y se testea contra cada una de las observaciones. En todos los casos el modelo se estima K veces, la diferencia radica en que valor toma ese K. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelos = [\"KNN\",\"LDA\",\"Logit\"]\n",
    "parametros = {\"k_range\": range(2,11), \"neighbors\":5, \"components\":1, \"penalty\": [\"l1\"], \"lambda\": np.logspace(-5,5,num=5-(-5)+1,base=10)} #Penalty de Lasso, la l2 es Ridge. \n",
    "\n",
    "\n",
    "\n",
    "reg_lasso = evalua_multiples_metodos(modelos,x,y,parametros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabla en la posición 1 del objeto reg_lasso, correspondiéndose con la tabla de métricas obtenida con la función evalua_multiples_metodos.\n",
    "reg_lasso[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabla con los distintos valores de lambda y sus respectivos ECM promedio, mostrando que los valores más pequeños parecen ser los mejores si buscamos minimizar el ECM. \n",
    "\n",
    "reg_lasso[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parametros = {\"k_range\": range(2,11), \"neighbors\":5, \"components\":1, \"penalty\": [\"l2\"], \"lambda\": np.logspace(-5,5,num=5-(-5)+1,base=10)} #Penalty de Ridge. \n",
    "\n",
    "reg_ridge = evalua_multiples_metodos(modelos,x,y,parametros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_ridge[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El valor de λ elegido para cada caso es de 1 para el caso de Lasso y 10 para el caso de Ridge. A su vez, en los box plots mostrados a continuación puede observarse que los menores valores de ECM se encuentran para los valores más chicos de lambda (a excepción del valor asociado con 100), dándose un salto discreto considerable entre los valores 100 y 1000 en LASSO y dos saltos diferentes en Ridge, un primer salto para valores entre 100 y 1000 y  otro  salto para valores entre 1000 y 10000 del hiperparámetro bajo análisis.\n",
    "\n",
    " Los box-plots nos muestran que el ECM se comporta de manera muy similar en los dos métodos. Hasta un alpha de 100, el ECM promedio para las K particiones se mantiene por debajo de 0.24, aunque el despegue en Lasso se da antes que en Ridge, con un alpha de 1, mientras que en Ridge, el primer salto más discreto se da con un alpha de 100. \n",
    " \n",
    " Otra diferencia entre los dos métodos es que cuando alpha llega a 1000, el ECM promedio en Lasso salta directamente a un nivel alrededor de 0.22, mientras que en Ridge su primer salto es más cercano 0.19 y su segundo salto más cercano a 0.22. Es decir a partir de 1000, los dos métodos llegan a un valor cercano a 0.22 y se mantiene constante .  \n",
    "\n",
    "Otro punto interesante es que, a medida que aumenta el valor de λ bajo Lasso, los intervalos entre los que se encuentra el ECM dentro de las K particiones son más amplios. Por otro lado, en Ridge sucede casi lo mismo a excepción del valor de 100 donde Ridge da el primer salto. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot para regularización LASSO\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "ss = sns.boxplot(data=reg_lasso[1], x=\"alp\", y=\"ecm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot para regularización Ridge\n",
    "\n",
    "sns.set()\n",
    "ss = sns.boxplot(data=reg_ridge[1], x=\"alp\", y=\"ecm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 6 (CORREGIR)\n",
    " LASSO elimina todos los coeficientes menos 1. Esto tiene sentido si pensamos que está eligiendo un valor de $\\lambda$ alto. Esto significaría que el parámetro C, la inversa de λ, es un valor muy bajo, lo que implica una regularización más fuerte.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=10)\n",
    "lasso_best = Lasso(alpha=1)\n",
    "lasso_best.fit(x_train, y_train)\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "print(list(zip(lasso_best.coef_, x)))\n",
    "\n",
    "coeficientes_finales = pd.DataFrame([np.array(x_train.columns), lasso_best.coef_]).T\n",
    "coeficientes_finales.columns = ['Variable', 'Coeficiente']\n",
    "print(f\"El modelo final cuenta con: {coeficientes_finales[coeficientes_finales['Coeficiente'] != 0].shape[0]} variables\")\n",
    "coeficientes_finales\n",
    "#Chequear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_best = Ridge(alpha=10)\n",
    "ridge_best.fit(x_train, y_train)\n",
    "\n",
    "lasso_mse = mean_squared_error(y_test, lasso_best.predict(x_test))\n",
    "ridge_mse = mean_squared_error(y_test, ridge_best.predict(x_test))\n",
    "\n",
    "print(\"Error cuadrático medio (Lasso): %.2f\" % lasso_mse)\n",
    "print(\"Error cuadrático medio (Ridge): %.2f\" % ridge_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar en nuestras respuestas anteriores, el error cuadrático medio de ridge es menor. Sin embargo, el modelo de Lasso elimina todas las variables que pensamos a priori serian relevantes menos una. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelos = [\"Logit\"]\n",
    "parametros = {\"k_range\": range(3,5), \"penalty\": [\"l1\", \"l2\"], \"lambda\": np.logspace(-5,5,num=5-(-5)+1,base=10)} # l1enalty de Lasso, la l2 es Ridge. \n",
    "\n",
    "lasso_7 = evalua_multiples_metodos(modelos,x,y,parametros)\n",
    "\n",
    "lasso_7[0]\n",
    "\n",
    "#Chequear que sea efectivamente Logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 8\n",
    "\n",
    "En base a las tablas presentadas en los incisos anteriores, si nuestra preocupación es minimizar el ECM y mejor la precisión, el mejor método de los analizados es LDA con un λ = 10, ya que el ECM en este caso es de 0.198758, contra un 0.195652 de LDA (con 1 componente) y 0.245342 de KNN (con 5 vecinos). \n",
    "\n",
    "Sin embargo, si la idea es identificar hogares pobres para incluirlos en alguna política, nuestro objetivo es  identificar la mayor cantidad posible de verdaderos positivos, ya que esto nos brinda un mayor alcance dentro de la población objetivo. Si tenemos en cuenta esto, LDA es un método superior, ya que identifica 31 verdaderos positivos al observar la matriz de confusión, contra 27 de Logit. La cantidad de verdaderos negativos solo difere en 3 a favor de Logit, por lo que algún análisis costo-beneficio no pareciera ir en contra de la mejor identificación de LDA. \n",
    "\n",
    "A su vez, el área debajo de la curva ROC de LDA es de 0.664024, contra un 0.645884 de Logit, por lo que marginalmente podemos ver que el método LDA posee una mejor precisión en la clasificación. Por esta razón y por lo expuesto en el párrafo anterior, decidimos que el mejor método resulta ser LDA con un $\\lambda = 10$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=10)\n",
    "\n",
    "lda_final = LinearDiscriminantAnalysis(n_components=1)\n",
    "lda_final_fit = lda_final.fit(x_train, y_train)\n",
    "\n",
    "y_pred = lda_final_fit.predict(x_test)\n",
    "y_pred = np.where(y_pred > 0.5, 1, y_pred)\n",
    "y_pred = np.where(y_pred <= 0.5, 0, y_pred)\n",
    "\n",
    "count = np.count_nonzero(y_pred == 1)\n",
    "\n",
    "print(\"En base al modelo\",lda_final,\", la cantidad predicha de hogares pobres para la submuestra es de\", count, \"personas.\")\n",
    "\n",
    "proporcion = np.count_nonzero(y_pred == 1)/(len(y_pred))\n",
    "proporcion = proporcion*100\n",
    "print(\"La proporción de hogares pobres en la submuestra es del %.2f\" % proporcion,\"%.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
